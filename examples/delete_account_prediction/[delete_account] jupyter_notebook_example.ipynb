{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Цель__\n",
    "\n",
    "Хотим понять места в приложении, которые непонятны пользователю и провоцируют удаление аккаунта.\n",
    "\n",
    "__Задачи__\n",
    "\n",
    "1. Собрать необходимые данные\n",
    "2. Подготовить данные для аналитики\n",
    "3. Провести анализ\n",
    "    1. Построить сводные таблицы\n",
    "    2. Визуализировать траектории\n",
    "    3. Построить классификатор\n",
    "        1. Помогает выделить специфичные для конкретной группы траектории\n",
    "        2. Позволяет по текущей траектории оценить вероятность удаление аккаунта пользователя, например, чтобы динамически менять контент приложения, чтобы люди оставались\n",
    "\n",
    "__Ожидаемые результаты__\n",
    "\n",
    "1. Узнать наиболее проблемные участки приложения, чтобы предотвратить уход юзеров\n",
    "2. Получить классификатор, который позволит предсказывать удаление аккаунта по тому, как пользователь использует приложение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import retentioneering\n",
    "from retentioneering.utils import download_events_multi, preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client, job_config = retentioneering.init_from_json('./settings.json')\n",
    "settings = retentioneering.Config('./settings.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### you can use one settings file in download_events_multi for multiple queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = download_events_multi(client, job_config=job_config, settings=settings)\n",
    "print 'Downloaded DataFrame shape: {}'.format(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It creates a new column 'group_name' with name of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.group_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = '../../data/data_from_bq_delete_accounts.csv'\n",
    "df.to_csv(path, sep=';', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now you can prepare your DataFrame for further analysis using pandas instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop duplicated events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preparing.drop_duplicated_events(df, settings=settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delete from test group users from deleted group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many users we have in each group\n",
    "df.groupby('group_name').user_pseudo_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in two groups\n",
    "df_deleted = df.loc[df.group_name == u'deleted', :].copy()\n",
    "df_test = df.loc[df.group_name == u'test_group', :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select users from 'deleted' group\n",
    "selected_users = df_deleted.user_pseudo_id.unique()\n",
    "\n",
    "# drop selected users from second group\n",
    "df_test = df_test.loc[~df_test.user_pseudo_id.isin(selected_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'deleted:', df_deleted.user_pseudo_id.nunique()\n",
    "print 'test_group:', df_test.user_pseudo_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### leave only users who delete their account in first two days and has 'first_open' event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave only users with first open\n",
    "has_first_open_users = df_deleted.loc[df_deleted.event_name == u'first_open', 'user_pseudo_id'].unique()\n",
    "df_deleted = df_deleted.loc[df_deleted.user_pseudo_id.isin(has_first_open_users), :]\n",
    "print 'deleted group users:', df_deleted.user_pseudo_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for each user find timestamp when account was deleted, and timestamp of first event\n",
    "delete_account_time = df_deleted.groupby('user_pseudo_id', as_index=False)['event_timestamp'].max()\n",
    "first_event_time = df_deleted.groupby('user_pseudo_id', as_index=False)['event_timestamp'].min()\n",
    "\n",
    "# calculate users lifetime\n",
    "users_lifetime = first_event_time.merge(delete_account_time, \n",
    "                                        how='left', \n",
    "                                        on='user_pseudo_id', \n",
    "                                        suffixes=('_min', '_del'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "days_threshold = 2\n",
    "# select users with timedelta 'days_threshold' days and leave only them\n",
    "selected_users = users_lifetime[(users_lifetime['event_timestamp_del'] -\n",
    "                users_lifetime['event_timestamp_min']) / 1e6 / (24*60*60) <= days_threshold].user_pseudo_id.unique()\n",
    "df_deleted = df_deleted.loc[df_deleted.user_pseudo_id.isin(selected_users), :]\n",
    "print 'deleted group users:', df_deleted.user_pseudo_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in second group leave only events within two days\n",
    "first_event_time = df_test.groupby('user_pseudo_id', as_index=False)['event_timestamp'].min()\n",
    "df_test = df_test.merge(first_event_time, how='left', on='user_pseudo_id', suffixes=('', '_min'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.loc[(df_test.event_timestamp - df_test.event_timestamp_min) / 1e6 / (24*60*60) <= days_threshold, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select same size of users from test dataset\n",
    "np.random.seed(seed=42)\n",
    "df = df_test.loc[df_test.user_pseudo_id.isin(\n",
    "    np.random.choice(df_test.user_pseudo_id.unique(), size=df_deleted.user_pseudo_id.nunique(), replace=False)), :] \\\n",
    "    .append(df_deleted, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/data_from_bq_delete_accounts_prepared_data.csv'\n",
    "df.to_csv(path, sep=';', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from retentioneering import analysis\n",
    "from retentioneering.utils import preparing\n",
    "path = '../../data_from_bq_delete_accounts_prepared_data.csv'\n",
    "df = pd.read_csv(path, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add accumulator events\n",
    "\n",
    "df = preparing.add_lost_events(\n",
    "    df, positive_event_name=u'settings_delete_account_success', negative_event_name=u'not_delete_account')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load filters for events\n",
    "event_filter = pd.read_csv('~/NewUserEventList.csv').values\n",
    "lexa_filter = pd.read_csv('~/lexa_filter.csv', ';')\n",
    "lexa_filter = lexa_filter[lexa_filter.Created_by == '1']['Event Action'].values\n",
    "lf = {'app_provisional_enabledPush', 'app_enabledPush','app_enabledRemotePush'}\n",
    "event_filter = set(lexa_filter)|set(event_filter[:, 0])|{'settings_delete_account_success', 'not_delete_account'}\n",
    "df = df[df.event_name.isin(event_filter - lf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only top-30 events for better visualization\n",
    "top_events = df.groupby('event_name').user_pseudo_id.count()\n",
    "top_events = set(top_events.sort_values().iloc[-20:].index)|set(['settings_delete_account_success', 'not_delete_account'])\n",
    "df = df[df.event_name.isin(top_events)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = analysis.get_desc_table(df, target_event_list=['settings_delete_account_success',\n",
    "                                                      'not_delete_account'],\n",
    "                               max_steps=30, settings=settings, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lost_users_list = df[df.event_name == 'settings_delete_account_success'].user_pseudo_id\n",
    "filt = df.user_pseudo_id.isin(lost_users_list)\n",
    "df_lost = df[filt]\n",
    "df_passed = df[~filt]\n",
    "\n",
    "desc_loss = analysis.get_desc_table(df_lost, target_event_list=['settings_delete_account_success',\n",
    "                                                                'not_delete_account'],\n",
    "                                    max_steps=30, settings=settings, plot=False)\n",
    "desc_passed = analysis.get_desc_table(df_passed, target_event_list=['settings_delete_account_success',\n",
    "                                                                    'not_delete_account'],\n",
    "                                      max_steps=30, settings=settings, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "diff_df = analysis.get_diff(desc_loss, desc_passed, settings=settings, precalc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot graph with python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = analysis.get_all_agg(df_lost, agg_list=['trans_count'])\n",
    "analysis.utils.plot_graph_python(df_agg, 'trans_count', settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or plot graph via api\n",
    "\n",
    "`It sends your data on our server`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retentioneering.utils.export import plot_graph_api\n",
    "plot_graph_api(df_lost, settings, task='delete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get raw data for model\n",
    "df = pd.read_csv(path, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "clf = analysis.Model(df, target_event='settings_delete_account_success', event_filter=event_filter, settings=settings)\n",
    "clf.fit_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find leaky-events by analysing model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = clf.model.coef_[0]\n",
    "names = clf._embedder.inverse_transform([importance])[0]\n",
    "importance = importance[importance != 0]\n",
    "list(zip(names, importance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can see that `settings_delete_account` and `settings` has much more importance then other, so we should add it to filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_filter = event_filter|set(['settings_delete_account', 'settings'])\n",
    "# and rebuild model\n",
    "clf = analysis.Model(df, target_event='settings_delete_account_success', event_filter=event_filter, settings=settings)\n",
    "clf.fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat it\n",
    "importance = clf.model.coef_[0]\n",
    "names = clf._embedder.inverse_transform([importance])[0]\n",
    "importance = importance[importance != 0]\n",
    "list(zip(names, importance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_filter = event_filter|set(['profile_settings'])\n",
    "clf = analysis.Model(df, target_event='settings_delete_account_success', event_filter=event_filter, settings=settings)\n",
    "clf.fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one more time\n",
    "importance = clf.model.coef_[0]\n",
    "names = clf._embedder.inverse_transform([importance])[0]\n",
    "importance = importance[importance != 0]\n",
    "list(zip(names, importance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all weights seems more uniformly distributed, so we can stop.\n",
    "\n",
    "We also can get most valued edges and nodes from model to visualize it on graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_tracks = clf.build_important_track()\n",
    "# edges\n",
    "imp_tracks[imp_tracks[1].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes\n",
    "imp_tracks[imp_tracks[1].isnull()][0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
